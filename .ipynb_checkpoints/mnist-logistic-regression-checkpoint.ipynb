{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Notes: \n",
    "#Updated one has gradient descent with rmsprop optimization, which gave an improvement of \n",
    "# 0.5%, which is good, I suppose. Also, small batch sizes give better results than large \n",
    "# batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the data from train file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist_data():\n",
    "    \n",
    "    data=pd.read_csv('train.csv', sep=',').as_matrix()\n",
    "    data.shape\n",
    "    N = data.shape[0]\n",
    "    n_train = int((N*90)/100)\n",
    "    X_train = np.column_stack((data[:n_train,1:], np.ones((n_train,1))))\n",
    "    Y_train = data[:n_train,0]\n",
    "    \n",
    "    X_test = np.column_stack((data[n_train:,1:], np.ones((N-n_train,1))))\n",
    "    Y_test = data[n_train:,0]\n",
    "\n",
    "    C = 10\n",
    "    d = X_train.shape[1]\n",
    "    return data, N, n_train, X_train, Y_train, X_test, Y_test, C, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the data into training and testing data\n",
    "data, N, n_train, X_train, Y_train, X_test, Y_test, C, d = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8EtFGX6pKMMvFfX/ViuIsyHvWZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11184ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the image just so that you can see how they are\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "n = 38\n",
    "imgplot = plt.imshow(X_train[n,:-1].reshape(28,28))\n",
    "Y_train[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theta = np.random.rand(C,d)/1000\n",
    "theta = np.zeros((C,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-10\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        \n",
    "        x[ix] = x[ix]+h\n",
    "        \n",
    "        random.setstate(rndstate)\n",
    "        fx1,grad1 = f(x)\n",
    "        numgrad = (fx1-fx)/h\n",
    "        \n",
    "        x[ix] = x[ix]-h\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print( \"Gradient check failed.\")\n",
    "            print( \"First gradient error found at index %s\" % str(ix))\n",
    "            print( \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "        \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print( \"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600,)\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "def compute_probability_matrix(theta, X):\n",
    "    prod = X.dot(theta.T)\n",
    "    exp = np.exp(prod-np.max(prod, axis=1).reshape(-1,1))\n",
    "    norm_const = np.sum(exp,axis=1).reshape(-1,1)\n",
    "    prob_matrix = exp/(norm_const)\n",
    "    return prob_matrix\n",
    "\n",
    "def compute_probability_array(theta, X, Y):\n",
    "    prob_matrix = compute_probability_matrix(theta, X)\n",
    "    k = prob_matrix.shape[0]\n",
    "    return prob_matrix[np.arange(k),Y]\n",
    "\n",
    "#computes sum log p(y=y(i)|x(i)) for all i belonging to the training set\n",
    "def compute_log_entropy(theta, X, Y):\n",
    "    prob_array = compute_probability_array(theta, X, Y)\n",
    "    N = X.shape[0]\n",
    "    return np.sum(np.log(prob_array))/N\n",
    "\n",
    "def compute_gradient(theta, X, Y):\n",
    "    prob_matrix = compute_probability_matrix(theta, X)\n",
    "    k = prob_matrix.shape[0]\n",
    "    prob_matrix[np.arange(k),Y] -= 1\n",
    "    N = X.shape[0]\n",
    "    return -prob_matrix.T.dot(X)/N\n",
    "\n",
    "def cost_grad(theta, X, Y):\n",
    "    cost = compute_log_entropy(theta, X, Y)\n",
    "    grad = compute_gradient(theta, X, Y)\n",
    "    return cost, grad\n",
    "\n",
    "def predict(theta, X):\n",
    "    prob_matrix = compute_probability_matrix(theta, X)\n",
    "    return np.argmax(prob_matrix, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "theta_toy = np.random.rand(2,2)\n",
    "X_toy = np.array([[1,2],[2,1],[3,2]])\n",
    "Y_toy = np.array([0,1,0])\n",
    "print(theta_toy.shape, X_toy.shape, Y_toy.shape)\n",
    "print (cost_grad(theta_toy, X_toy, Y_toy))\n",
    "gradcheck_naive(lambda t : cost_grad(t, X_toy, Y_toy), theta_toy, 1e-6)\n",
    "\"\"\"\n",
    "print(Y_train.shape)\n",
    "gradcheck_naive(lambda t : cost_grad(t, X_train[1:10], Y_train[1:10]), theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\t Training cost -2.289582\t Testing cost -2.295088\t\n",
      "Iteration 1000\t Training cost -0.699849\t Testing cost -0.695688\t\n",
      "Iteration 2000\t Training cost -0.519950\t Testing cost -0.519113\t\n",
      "Iteration 3000\t Training cost -0.450567\t Testing cost -0.450740\t\n",
      "Iteration 4000\t Training cost -0.409157\t Testing cost -0.411747\t\n",
      "Iteration 5000\t Training cost -0.388549\t Testing cost -0.394830\t\n",
      "Iteration 6000\t Training cost -0.372130\t Testing cost -0.376001\t\n",
      "Iteration 7000\t Training cost -0.358801\t Testing cost -0.362374\t\n",
      "Iteration 8000\t Training cost -0.347793\t Testing cost -0.355436\t\n",
      "Iteration 9000\t Training cost -0.341664\t Testing cost -0.348668\t\n",
      "Iteration 10000\t Training cost -0.338168\t Testing cost -0.345758\t\n",
      "Iteration 11000\t Training cost -0.332961\t Testing cost -0.338602\t\n",
      "Iteration 12000\t Training cost -0.327907\t Testing cost -0.337320\t\n",
      "Iteration 13000\t Training cost -0.324478\t Testing cost -0.336795\t\n",
      "Iteration 14000\t Training cost -0.321115\t Testing cost -0.334412\t\n",
      "Iteration 15000\t Training cost -0.319701\t Testing cost -0.329513\t\n",
      "Iteration 16000\t Training cost -0.316852\t Testing cost -0.330416\t\n",
      "Iteration 17000\t Training cost -0.313970\t Testing cost -0.327853\t\n",
      "Iteration 18000\t Training cost -0.311664\t Testing cost -0.323370\t\n",
      "Iteration 19000\t Training cost -0.308970\t Testing cost -0.324351\t\n",
      "Iteration 20000\t Training cost -0.305844\t Testing cost -0.320356\t\n",
      "Iteration 21000\t Training cost -0.308045\t Testing cost -0.323896\t\n",
      "Iteration 22000\t Training cost -0.304164\t Testing cost -0.320613\t\n",
      "Iteration 23000\t Training cost -0.306261\t Testing cost -0.320091\t\n",
      "Iteration 24000\t Training cost -0.305245\t Testing cost -0.323049\t\n",
      "Iteration 25000\t Training cost -0.302315\t Testing cost -0.318754\t\n",
      "Iteration 26000\t Training cost -0.302771\t Testing cost -0.316594\t\n",
      "Iteration 27000\t Training cost -0.301015\t Testing cost -0.314633\t\n",
      "Iteration 28000\t Training cost -0.301071\t Testing cost -0.318300\t\n",
      "Iteration 29000\t Training cost -0.301424\t Testing cost -0.319585\t\n",
      "Iteration 30000\t Training cost -0.305892\t Testing cost -0.323978\t\n",
      "Iteration 31000\t Training cost -0.300649\t Testing cost -0.315104\t\n",
      "Iteration 32000\t Training cost -0.299263\t Testing cost -0.314396\t\n",
      "Iteration 33000\t Training cost -0.296630\t Testing cost -0.312850\t\n",
      "Iteration 34000\t Training cost -0.301831\t Testing cost -0.320645\t\n",
      "Iteration 35000\t Training cost -0.297993\t Testing cost -0.318280\t\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDX8ydkIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111dac208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.1904761905 91.5529100529\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(f, predict, theta, Xr, Yr, Xe, Ye, batch_size=5, max_iterations=100, print_iteration=10, learning_rate=0.0001):\n",
    "    N_train = Xr.shape[0]\n",
    "    batch_count = int(N_train/batch_size)\n",
    "    it = 0\n",
    "    training_error = []\n",
    "    testing_error = []\n",
    "    cache = np.zeros((theta.shape))\n",
    "    while it < max_iterations:\n",
    "        #pick a batch\n",
    "        batch = random.randint(0, batch_count)\n",
    "        if(batch < batch_count-1):\n",
    "            X, Y = Xr[batch:batch+batch_size], Yr[batch:batch+batch_size]\n",
    "        else:\n",
    "            X, Y = Xr[batch:], Yr[batch:]\n",
    "        \n",
    "        cost, grad = f(theta, X, Y)\n",
    "        a = 0.99\n",
    "        cache = (1-a)*grad**2 + a*cache\n",
    "        theta += learning_rate*grad /(np.sqrt(cache) + 1e-7)\n",
    "        \n",
    "        if it%print_iteration == 0:\n",
    "            training_cost = f(theta, Xr, Yr)[0]\n",
    "            testing_cost = f(theta, Xe, Ye)[0]\n",
    "            training_error.append(training_cost)\n",
    "            testing_error.append(testing_cost)\n",
    "            print(\"Iteration %d\\t Training cost %f\\t Testing cost %f\\t\"%(it, training_cost, testing_cost))\n",
    "        \n",
    "        it += 1\n",
    "    \n",
    "    Y_testp = predict(theta, Xe)\n",
    "    Y_trainp = predict(theta, Xr)\n",
    "\n",
    "    test_accuracy = np.mean(Y_testp==Ye)*100\n",
    "    train_accuracy = np.mean(Y_trainp==Yr)*100\n",
    "    return training_error, testing_error, theta, train_accuracy, test_accuracy\n",
    "\n",
    "theta = np.zeros((C,d))\n",
    "training_error, testing_error, theta, train_accuracy, test_accuracy = gradient_descent(cost_grad, predict, theta, X_train, Y_train, X_test, Y_test,\n",
    "                                                        print_iteration=1000, max_iterations=36000,learning_rate=1e-6)\n",
    "\n",
    "x = range(len(training_error))\n",
    "plt.plot(x, training_error, x, testing_error)\n",
    "plt.show()\n",
    "\n",
    "print(test_accuracy, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.1666666667 91.0386904762\n"
     ]
    }
   ],
   "source": [
    "Y_testp = predict(theta, X_test)\n",
    "Y_trainp = predict(theta, X_train)\n",
    "\n",
    "test_accuracy = np.mean(Y_testp==Y_test)*100\n",
    "train_accuracy = np.mean(Y_trainp==Y_train)*100\n",
    "print(test_accuracy, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_Kaggle=pd.read_csv('test.csv', sep=',').as_matrix()\n",
    "X_Kaggle = np.column_stack((X_Kaggle, np.ones((28000,1))))\n",
    "Y_Kaggle = predict(theta, X_Kaggle)\n",
    "dataframe = pd.DataFrame({\"ImageId\":np.arange(28000)+1,\"Label\":Y_Kaggle})\n",
    "dataframe.to_csv('submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.7884114583 99.7380239521\n",
      "99.0034364261 98.5536359984\n",
      "99.1487311275 99.2789373814\n",
      "99.4748433 99.0310077519\n",
      "98.5893168489 98.4959647836\n",
      "94.6532999165 94.4946913095\n",
      "99.6004863644 99.4278708623\n",
      "99.5619016713 99.5372155804\n",
      "97.7050864874 97.9668049793\n",
      "99.0712597096 98.8804478209\n",
      "98.2094411286 97.3728106756\n",
      "99.1238416175 99.2531446541\n",
      "96.3286401711 95.3833121559\n",
      "91.1837021426 90.7422512235\n",
      "98.3777937996 97.6714100906\n",
      "98.3561169753 97.9518072289\n",
      "99.5796281326 99.6206373293\n",
      "97.9013820167 97.2686506319\n",
      "99.0410497981 98.820754717\n",
      "98.6029665402 97.926171713\n",
      "96.4811490126 95.5546147333\n",
      "99.4274166386 99.3063583815\n",
      "98.8335435057 99.0514410799\n",
      "97.5083056478 97.8498827209\n",
      "98.1648369654 97.885994715\n",
      "97.985563203 97.6152623211\n",
      "99.0225170187 98.7434130523\n",
      "99.6822211072 99.6483001172\n",
      "99.280322977 99.0392313851\n",
      "97.378338522 97.3146747352\n",
      "95.4663436581 95.3677366924\n",
      "93.7052200614 93.3385579937\n",
      "98.8278516445 98.1810665564\n",
      "94.3695335277 93.9662447257\n",
      "98.3966538864 98.2940698619\n",
      "98.2363913855 98.2859368913\n",
      "99.2394122731 98.6587771203\n",
      "99.2731384267 99.4032077583\n",
      "98.0910175558 98.0784627702\n",
      "97.9495798319 97.3735032831\n",
      "93.9179875948 93.7703583062\n",
      "96.6463414634 96.5101786456\n",
      "99.6910401648 99.5198079232\n",
      "92.1303258145 92.2811059908\n",
      "96.2743732591 96.2903869166\n",
      "Train Accuracy 90.098639, \tTest Accuracy 89.357143\n"
     ]
    }
   ],
   "source": [
    "#pairwise classification\n",
    "def matching_indices(Y, i, j, n):\n",
    "    x = [p for p in range(n) if Y[p] == i]\n",
    "    y = [p for p in range(n) if Y[p] == j]\n",
    "    return x,y\n",
    "\n",
    "params = []\n",
    "n_test = N-n_train\n",
    "i = 0\n",
    "j = 4\n",
    "for i in range(10):\n",
    "    params.append([])\n",
    "    for j in range(i):\n",
    "        theta = np.zeros((2,d))\n",
    "        x, y = matching_indices(Y_train, i, j, n_train)\n",
    "        x_t, y_t = matching_indices(Y_test, i, j, n_test)\n",
    "        xy = x+y\n",
    "        xy_t = x_t+y_t\n",
    "        Y_train_xy = np.copy(Y_train[xy])\n",
    "        X_train_xy = np.copy(X_train[xy])\n",
    "        Y_test_xy = np.copy(Y_test[xy_t])\n",
    "        X_test_xy = np.copy(X_test[xy_t])\n",
    "\n",
    "        x, y = matching_indices(Y_train_xy, i, j, Y_train_xy.shape[0])\n",
    "        x_t, y_t = matching_indices(Y_test_xy, i, j, Y_test_xy.shape[0])\n",
    "        Y_train_xy[x] = 0\n",
    "        Y_train_xy[y] = 1\n",
    "        Y_test_xy[x_t] = 0\n",
    "        Y_test_xy[y_t] = 1\n",
    "        training_error, testing_error, theta, train_accuracy, test_accuracy = gradient_descent(cost_grad, predict, theta, X_train_xy, Y_train_xy,\n",
    "                                                                X_test_xy, Y_test_xy, \n",
    "                                                                print_iteration=1000000, max_iterations=100000,learning_rate=1e-5)\n",
    "\n",
    "        x = range(len(training_error))\n",
    "        #plt.plot(x, training_error, x, testing_error)\n",
    "        #plt.show()\n",
    "        params[i].append(theta)\n",
    "        print(train_accuracy, test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 91.142857, \tTest Accuracy 90.460317\n"
     ]
    }
   ],
   "source": [
    "def predict_pairwise(params, X):\n",
    "    N = X.shape[0]\n",
    "    Y = np.zeros((N,d))\n",
    "    for i in range(10):\n",
    "        for j in range(i):\n",
    "            prob_matrix = compute_probability_matrix(params[i][j], X)\n",
    "            k = [k for k in range(N) if prob_matrix[k][0] > 0.8]\n",
    "            l = [l for l in range(N) if prob_matrix[l][1] > 0.8]\n",
    "            Y[k,i] += 1\n",
    "            Y[l,j] += 1\n",
    "            \n",
    "    return np.argmax(Y, axis=1)\n",
    "\n",
    "Y_testp = predict_pairwise(params, X_test)\n",
    "Y_trainp = predict_pairwise(params, X_train)\n",
    "\n",
    "test_accuracy = np.mean(Y_testp==Y_test)*100\n",
    "train_accuracy = np.mean(Y_trainp==Y_train)*100\n",
    "print(\"Train Accuracy %f, \\tTest Accuracy %f\"%(train_accuracy,test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_Kaggle=pd.read_csv('test.csv', sep=',').as_matrix()\n",
    "X_Kaggle = np.column_stack((X_Kaggle, np.ones((28000,1))))\n",
    "Y_Kaggle = predict_pairwise(params, X_Kaggle)\n",
    "dataframe = pd.DataFrame({\"ImageId\":np.arange(28000)+1,\"Label\":Y_Kaggle})\n",
    "dataframe.to_csv('submit_pairwise.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
