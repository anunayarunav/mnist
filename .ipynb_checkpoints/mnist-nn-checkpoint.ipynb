{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data from train file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def load_mnist_data():\n",
    "    \n",
    "    data=pd.read_csv('train.csv', sep=',').as_matrix()\n",
    "    data.shape\n",
    "    N = data.shape[0]\n",
    "    n_train = int((N*70)/100)\n",
    "    X_train = data[:n_train,1:]\n",
    "    Y_train = data[:n_train,0]\n",
    "\n",
    "    X_test = data[n_train:,1:]\n",
    "    Y_test = data[n_train:,0]\n",
    "\n",
    "    C = 10\n",
    "    d = X_train.shape[1]\n",
    "    return data, N, n_train, X_train, Y_train, X_test, Y_test, C, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, N, n_train, X_train, Y_train, X_test, Y_test, C, d = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-10\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        \n",
    "        x[ix] = x[ix]+h\n",
    "        \n",
    "        random.setstate(rndstate)\n",
    "        fx1,grad1 = f(x)\n",
    "        numgrad = (fx1-fx)/h\n",
    "        \n",
    "        x[ix] = x[ix]-h\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print( \"Gradient check failed.\")\n",
    "            print( \"First gradient error found at index %s\" % str(ix))\n",
    "            print( \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "        \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print( \"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def network_forward_backward_pass(X, y, W1, b1, W2, b2, reg = 1e-3):\n",
    "    N = X.shape[0]\n",
    "    H = np.dot(X, W1) + b1\n",
    "    H_relu = np.maximum(H, 0)\n",
    "    scores = np.dot(H_relu, W2) + b2\n",
    "    \n",
    "    if y is None:\n",
    "        return scores\n",
    "    \n",
    "    weights = scores - np.max(scores, axis=1, keepdims=True)\n",
    "    exp = np.exp(weights)\n",
    "    prob_matrix = exp/np.sum(exp, axis=1, keepdims=True)\n",
    "    loss = -np.sum(np.log(prob_matrix[np.arange(N), y.reshape(1,-1)]))\n",
    "    loss /= N\n",
    "    loss += 0.5 * reg * (np.sum(W1*W1) + np.sum(W2*W2))\n",
    "    \n",
    "    #print(prob_matrix[np.arange(N), y.reshape(1,-1)].shape)\n",
    "    prob_matrix[np.arange(N), y.reshape(1,-1)] -= 1\n",
    "    dW2 = H_relu.T.dot(prob_matrix)\n",
    "    db2 = np.sum(prob_matrix, axis=0)\n",
    "    dX = prob_matrix.dot(W2.T)\n",
    "    dH = dX*(H>0)\n",
    "    dW1 = X.T.dot(dH)\n",
    "    db1 = np.sum(dH, axis=0)\n",
    "    \n",
    "    dW2 /= N\n",
    "    db2 /= N\n",
    "    dW1 /= N\n",
    "    db1 /= N\n",
    "    \n",
    "    dW2 += reg * W2\n",
    "    dW1 += reg * W1\n",
    "    \n",
    "    return loss, dW1, db1, dW2, db2\n",
    "\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    return np.argmax(network_forward_backward_pass(X, None, W1, b1, W2, b2), axis=1)\n",
    "\n",
    "predict(X[1:5], W1, b1, W2, b2)\n",
    "#network_forward_backward_pass(X[1:5], y[1:5], W1, b1, W2, b2)\n",
    "#gradcheck_naive(lambda t : network_forward_backward_pass(X[1:5], y[1:5], W1, b1, W2, t), b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Cost: 2.280052\t Val cost 2.283979\n",
      "Train Cost: 0.494699\t Val cost 0.494338\n",
      "Train Cost: 0.397826\t Val cost 0.391848\n",
      "Train Cost: 0.373170\t Val cost 0.363433\n",
      "Train Cost: 0.333691\t Val cost 0.324535\n",
      "Train Cost: 0.324017\t Val cost 0.311131\n",
      "Train Cost: 0.318321\t Val cost 0.304616\n",
      "Train Cost: 0.287764\t Val cost 0.274572\n",
      "Train Cost: 0.287445\t Val cost 0.270577\n",
      "Train Cost: 0.268818\t Val cost 0.252219\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112413898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.926769\t Testing Accuracy 0.921587\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "y = Y_train\n",
    "X_val = X_test\n",
    "y_val = Y_test\n",
    "\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "H = 15\n",
    "C = 10\n",
    "std = 1e-2\n",
    "W1 = std*np.random.randn(D, H)\n",
    "b1 = np.zeros(H)\n",
    "\n",
    "W2 = std*np.random.randn(H, C)\n",
    "b2 = np.zeros(C)\n",
    "\n",
    "def gradient_descent_nn(X, y, X_val, y_val, W1, b1, W2, b2, batch_size = 200, \n",
    "                       lr = 1e-3, reg = 0.5e-2, num_iters = 100, print_iters = 10):\n",
    "    it = 0\n",
    "    N = X.shape[0]\n",
    "    stats_train = []\n",
    "    stats_val = []\n",
    "    best_val, best_params = -1, None\n",
    "    while it < num_iters:\n",
    "        indices = np.random.choice(N, batch_size)\n",
    "        X_batch = X[indices]\n",
    "        y_batch = y[indices]\n",
    "        loss, dW1, db1, dW2, db2 = network_forward_backward_pass(X_batch, y_batch, W1, b1, W2, b2, reg)\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "        \n",
    "        if it%print_iters == 0:\n",
    "            #train_acc = np.mean(y == predict(X, W1, b1, W2, b2))\n",
    "            #val_acc = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "            loss_val = network_forward_backward_pass(X, y, W1, b1, W2, b2, reg)[0]\n",
    "            loss_train = network_forward_backward_pass(X_val, y_val, W1, b1, W2, b2, reg)[0]\n",
    "            \n",
    "            test_accuracy = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "            if test_accuracy > best_val:\n",
    "                best_val = test_accuracy\n",
    "                best_params = (W1, b1, W2, b2)\n",
    "            \n",
    "            stats_train.append(loss_train)\n",
    "            stats_val.append(loss_val)\n",
    "            print(\"Train Cost: %f\\t Val cost %f\"%(loss_train, loss_val))\n",
    "            \n",
    "        it += 1\n",
    "    \n",
    "    return W1, b1, W2, b2, stats_train, stats_val, best_val, best_params\n",
    "\n",
    "W1, b1, W2, b2, training_error, testing_error, best_val, best_params = gradient_descent_nn(X, y, X_val, y_val, \n",
    "                                                                    W1, b1, W2, b2, \n",
    "                                                                    num_iters = 1000, print_iters = 100)\n",
    "\n",
    "x = range(len(training_error))\n",
    "plt.plot(x, training_error, x, testing_error)\n",
    "plt.show()\n",
    "\n",
    "test_accuracy = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "train_accuracy = np.mean(y == predict(X, W1, b1, W2, b2))\n",
    "print(\"Training Accuracy %f\\t Testing Accuracy %f\"%(train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for hidden size : 800, reg : 0.000010, lr : 0.001000, batch_size : 200\n",
      "Train Cost: 2.311556\t Val cost 2.312876\n",
      "Train Cost: 0.444912\t Val cost 0.009691\n",
      "Train Cost: 0.612937\t Val cost 0.001856\n",
      "Train Cost: 0.692384\t Val cost 0.000972\n",
      "Train Cost: 0.743995\t Val cost 0.000652\n",
      "Train Cost: 0.783001\t Val cost 0.000493\n",
      "Train Cost: 0.811884\t Val cost 0.000397\n",
      "Train Cost: 0.837040\t Val cost 0.000335\n",
      "Train Cost: 0.859262\t Val cost 0.000293\n",
      "Train Cost: 0.877138\t Val cost 0.000262\n",
      "Train Cost: 0.893554\t Val cost 0.000238\n",
      "Train Cost: 0.907907\t Val cost 0.000220\n",
      "Train Cost: 0.920791\t Val cost 0.000205\n",
      "Train Cost: 0.932948\t Val cost 0.000193\n",
      "Train Cost: 0.943899\t Val cost 0.000183\n"
     ]
    }
   ],
   "source": [
    "def train_with_hyperparameters(H, reg, lr, batch_size):\n",
    "    \n",
    "    print(\"Testing for hidden size : %d, reg : %f, lr : %f, batch_size : %d\"%(H, reg, lr, batch_size))\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    H = 15\n",
    "    C = 10\n",
    "    std = 1e-2\n",
    "    W1 = std*np.random.randn(D, H)\n",
    "    b1 = np.zeros(H)\n",
    "\n",
    "    W2 = std*np.random.randn(H, C)\n",
    "    b2 = np.zeros(C)\n",
    "\n",
    "    W1, b1, W2, b2, train_err, testing_err = gradient_descent_nn(X, y, X_val, y_val, W1, b1, W2, b2, \n",
    "                                                                 batch_size = batch_size, lr = lr, reg = reg, \n",
    "                                                                 num_iters = 1000000, print_iters = 50000)\n",
    "    \n",
    "    x = range(len(training_error))\n",
    "    plt.plot(x, training_error, x, testing_error)\n",
    "    plt.show()\n",
    "\n",
    "    test_accuracy = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "    train_accuracy = np.mean(y == predict(X, W1, b1, W2, b2))\n",
    "    print(\"Training Accuracy %f\\t Testing Accuracy %f\"%(train_accuracy, test_accuracy))\n",
    "    \n",
    "    params = [W1, b1, W2, b2, H, reg, lr, batch_size]\n",
    "    return params, train_accuracy, test_accuracy\n",
    "\n",
    "Hs = {800} #, 10, 15, 20, 25}\n",
    "regs = {1e-5, 1e-4} #, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1}\n",
    "lrs = {1e-3}\n",
    "batches = {200}\n",
    "\n",
    "best_val = -1\n",
    "best_params = None\n",
    "\n",
    "for H in Hs:\n",
    "    for reg in regs:\n",
    "        for lr in lrs:\n",
    "            for batch_size in batches:\n",
    "                params, train_accuracy, test_accuracy = train_with_hyperparameters(H, reg, lr, batch_size)\n",
    "                if test_accuracy > best_val:\n",
    "                    best_val = test_accuracy\n",
    "                    best_params = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_Kaggle=pd.read_csv('test.csv', sep=',').as_matrix()\n",
    "Y_Kaggle = predict(X_Kaggle, W1, b1, W2, b2)\n",
    "dataframe = pd.DataFrame({\"ImageId\":np.arange(28000)+1,\"Label\":Y_Kaggle})\n",
    "dataframe.to_csv('submit_nn.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(W1.shape)\n",
    "for i in range(H):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(W1[:,i].reshape(28,28))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
