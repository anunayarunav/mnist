{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Notes: \n",
    "#Updated one has gradient descent with rmsprop optimization, which gave an improvement of \n",
    "# 0.5%, which is good, I suppose. Also, small batch sizes give better results than large \n",
    "# batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the data from train file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist_data():\n",
    "    \n",
    "    data=pd.read_csv('train.csv', sep=',').as_matrix()\n",
    "    data.shape\n",
    "    N = data.shape[0]\n",
    "    n_train = int((N*90)/100)\n",
    "    X_train = np.column_stack((data[:n_train,1:], np.ones((n_train,1))))\n",
    "    Y_train = data[:n_train,0]\n",
    "    \n",
    "    X_test = np.column_stack((data[n_train:,1:], np.ones((N-n_train,1))))\n",
    "    Y_test = data[n_train:,0]\n",
    "\n",
    "    C = 10\n",
    "    d = X_train.shape[1]\n",
    "    return data, N, n_train, X_train, Y_train, X_test, Y_test, C, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the data into training and testing data\n",
    "data, N, n_train, X_train, Y_train, X_test, Y_test, C, d = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8EtFGX6pKMMvFfX/ViuIsyHvWZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11172b470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the image just so that you can see how they are\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "n = 38\n",
    "imgplot = plt.imshow(X_train[n,:-1].reshape(28,28))\n",
    "Y_train[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theta = np.random.rand(C,d)/1000\n",
    "theta = np.zeros((C,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-10\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        \n",
    "        x[ix] = x[ix]+h\n",
    "        \n",
    "        random.setstate(rndstate)\n",
    "        fx1,grad1 = f(x)\n",
    "        numgrad = (fx1-fx)/h\n",
    "        \n",
    "        x[ix] = x[ix]-h\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print( \"Gradient check failed.\")\n",
    "            print( \"First gradient error found at index %s\" % str(ix))\n",
    "            print( \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "        \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print( \"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37800,)\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "def compute_probability_matrix(theta, X):\n",
    "    prod = X.dot(theta.T)\n",
    "    exp = np.exp(prod-np.max(prod, axis=1).reshape(-1,1))\n",
    "    norm_const = np.sum(exp,axis=1).reshape(-1,1)\n",
    "    prob_matrix = exp/(norm_const)\n",
    "    return prob_matrix\n",
    "\n",
    "def compute_probability_array(theta, X, Y):\n",
    "    prob_matrix = compute_probability_matrix(theta, X)\n",
    "    k = prob_matrix.shape[0]\n",
    "    return prob_matrix[np.arange(k),Y]\n",
    "\n",
    "#computes sum log p(y=y(i)|x(i)) for all i belonging to the training set\n",
    "def compute_log_entropy(theta, X, Y):\n",
    "    prob_array = compute_probability_array(theta, X, Y)\n",
    "    N = X.shape[0]\n",
    "    return np.sum(np.log(prob_array))/N\n",
    "\n",
    "def compute_gradient(theta, X, Y):\n",
    "    prob_matrix = compute_probability_matrix(theta, X)\n",
    "    k = prob_matrix.shape[0]\n",
    "    prob_matrix[np.arange(k),Y] -= 1\n",
    "    N = X.shape[0]\n",
    "    return -prob_matrix.T.dot(X)/N\n",
    "\n",
    "def cost_grad(theta, X, Y):\n",
    "    cost = compute_log_entropy(theta, X, Y)\n",
    "    grad = compute_gradient(theta, X, Y)\n",
    "    return cost, grad\n",
    "\n",
    "def predict(theta, X):\n",
    "    prob_matrix = compute_probability_matrix(theta, X)\n",
    "    return np.argmax(prob_matrix, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "theta_toy = np.random.rand(2,2)\n",
    "X_toy = np.array([[1,2],[2,1],[3,2]])\n",
    "Y_toy = np.array([0,1,0])\n",
    "print(theta_toy.shape, X_toy.shape, Y_toy.shape)\n",
    "print (cost_grad(theta_toy, X_toy, Y_toy))\n",
    "gradcheck_naive(lambda t : cost_grad(t, X_toy, Y_toy), theta_toy, 1e-6)\n",
    "\"\"\"\n",
    "print(Y_train.shape)\n",
    "gradcheck_naive(lambda t : cost_grad(t, X_train[1:10], Y_train[1:10]), theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\t Training cost -2.174744\t Testing cost -2.171365\t\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(f, predict, theta, Xr, Yr, Xe, Ye, batch_size=5, max_iterations=100, print_iteration=10, learning_rate=0.0001):\n",
    "    N_train = Xr.shape[0]\n",
    "    batch_count = int(N_train/batch_size)\n",
    "    it = 0\n",
    "    training_error = []\n",
    "    testing_error = []\n",
    "    cache = np.zeros((theta.shape))\n",
    "    while it < max_iterations:\n",
    "        #pick a batch\n",
    "        batch = np.random.choice(N_train, batch_count)\n",
    "        X, Y = Xr[batch], Yr[batch]\n",
    "        \n",
    "        cost, grad = f(theta, X, Y)\n",
    "        a = 0.99\n",
    "        cache = (1-a)*grad**2 + a*cache\n",
    "        theta += learning_rate*grad /(np.sqrt(cache) + 1e-7)\n",
    "        \n",
    "        if it%print_iteration == 0:\n",
    "            training_cost = f(theta, Xr, Yr)[0]\n",
    "            testing_cost = f(theta, Xe, Ye)[0]\n",
    "            training_error.append(training_cost)\n",
    "            testing_error.append(testing_cost)\n",
    "            print(\"Iteration %d\\t Training cost %f\\t Testing cost %f\\t\"%(it, training_cost, testing_cost))\n",
    "        \n",
    "        it += 1\n",
    "    \n",
    "    Y_testp = predict(theta, Xe)\n",
    "    Y_trainp = predict(theta, Xr)\n",
    "\n",
    "    test_accuracy = np.mean(Y_testp==Ye)*100\n",
    "    train_accuracy = np.mean(Y_trainp==Yr)*100\n",
    "    return training_error, testing_error, theta, train_accuracy, test_accuracy\n",
    "\n",
    "theta = np.zeros((C,d))\n",
    "training_error, testing_error, theta, train_accuracy, test_accuracy = gradient_descent(cost_grad, predict, theta, X_train, Y_train, X_test, Y_test,\n",
    "                                                        print_iteration=1000, max_iterations=36000,learning_rate=1e-6)\n",
    "\n",
    "x = range(len(training_error))\n",
    "plt.plot(x, training_error, x, testing_error)\n",
    "plt.show()\n",
    "\n",
    "print(test_accuracy, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.1666666667 91.0386904762\n"
     ]
    }
   ],
   "source": [
    "Y_testp = predict(theta, X_test)\n",
    "Y_trainp = predict(theta, X_train)\n",
    "\n",
    "test_accuracy = np.mean(Y_testp==Y_test)*100\n",
    "train_accuracy = np.mean(Y_trainp==Y_train)*100\n",
    "print(test_accuracy, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_Kaggle=pd.read_csv('test.csv', sep=',').as_matrix()\n",
    "X_Kaggle = np.column_stack((X_Kaggle, np.ones((28000,1))))\n",
    "Y_Kaggle = predict(theta, X_Kaggle)\n",
    "dataframe = pd.DataFrame({\"ImageId\":np.arange(28000)+1,\"Label\":Y_Kaggle})\n",
    "dataframe.to_csv('submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.7884114583 99.7380239521\n",
      "99.0034364261 98.5536359984\n",
      "99.1487311275 99.2789373814\n",
      "99.4748433 99.0310077519\n",
      "98.5893168489 98.4959647836\n",
      "94.6532999165 94.4946913095\n",
      "99.6004863644 99.4278708623\n",
      "99.5619016713 99.5372155804\n",
      "97.7050864874 97.9668049793\n",
      "99.0712597096 98.8804478209\n",
      "98.2094411286 97.3728106756\n",
      "99.1238416175 99.2531446541\n",
      "96.3286401711 95.3833121559\n",
      "91.1837021426 90.7422512235\n",
      "98.3777937996 97.6714100906\n",
      "98.3561169753 97.9518072289\n",
      "99.5796281326 99.6206373293\n",
      "97.9013820167 97.2686506319\n",
      "99.0410497981 98.820754717\n",
      "98.6029665402 97.926171713\n",
      "96.4811490126 95.5546147333\n",
      "99.4274166386 99.3063583815\n",
      "98.8335435057 99.0514410799\n",
      "97.5083056478 97.8498827209\n",
      "98.1648369654 97.885994715\n",
      "97.985563203 97.6152623211\n",
      "99.0225170187 98.7434130523\n",
      "99.6822211072 99.6483001172\n",
      "99.280322977 99.0392313851\n",
      "97.378338522 97.3146747352\n",
      "95.4663436581 95.3677366924\n",
      "93.7052200614 93.3385579937\n",
      "98.8278516445 98.1810665564\n",
      "94.3695335277 93.9662447257\n",
      "98.3966538864 98.2940698619\n",
      "98.2363913855 98.2859368913\n",
      "99.2394122731 98.6587771203\n",
      "99.2731384267 99.4032077583\n",
      "98.0910175558 98.0784627702\n",
      "97.9495798319 97.3735032831\n",
      "93.9179875948 93.7703583062\n",
      "96.6463414634 96.5101786456\n",
      "99.6910401648 99.5198079232\n",
      "92.1303258145 92.2811059908\n",
      "96.2743732591 96.2903869166\n",
      "Train Accuracy 90.098639, \tTest Accuracy 89.357143\n"
     ]
    }
   ],
   "source": [
    "#pairwise classification\n",
    "def matching_indices(Y, i, j, n):\n",
    "    x = [p for p in range(n) if Y[p] == i]\n",
    "    y = [p for p in range(n) if Y[p] == j]\n",
    "    return x,y\n",
    "\n",
    "params = []\n",
    "n_test = N-n_train\n",
    "i = 0\n",
    "j = 4\n",
    "for i in range(10):\n",
    "    params.append([])\n",
    "    for j in range(i):\n",
    "        theta = np.zeros((2,d))\n",
    "        x, y = matching_indices(Y_train, i, j, n_train)\n",
    "        x_t, y_t = matching_indices(Y_test, i, j, n_test)\n",
    "        xy = x+y\n",
    "        xy_t = x_t+y_t\n",
    "        Y_train_xy = np.copy(Y_train[xy])\n",
    "        X_train_xy = np.copy(X_train[xy])\n",
    "        Y_test_xy = np.copy(Y_test[xy_t])\n",
    "        X_test_xy = np.copy(X_test[xy_t])\n",
    "\n",
    "        x, y = matching_indices(Y_train_xy, i, j, Y_train_xy.shape[0])\n",
    "        x_t, y_t = matching_indices(Y_test_xy, i, j, Y_test_xy.shape[0])\n",
    "        Y_train_xy[x] = 0\n",
    "        Y_train_xy[y] = 1\n",
    "        Y_test_xy[x_t] = 0\n",
    "        Y_test_xy[y_t] = 1\n",
    "        training_error, testing_error, theta, train_accuracy, test_accuracy = gradient_descent(cost_grad, predict, theta, X_train_xy, Y_train_xy,\n",
    "                                                                X_test_xy, Y_test_xy, \n",
    "                                                                print_iteration=1000000, max_iterations=100000,learning_rate=1e-5)\n",
    "\n",
    "        x = range(len(training_error))\n",
    "        #plt.plot(x, training_error, x, testing_error)\n",
    "        #plt.show()\n",
    "        params[i].append(theta)\n",
    "        print(train_accuracy, test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 91.142857, \tTest Accuracy 90.460317\n"
     ]
    }
   ],
   "source": [
    "def predict_pairwise(params, X):\n",
    "    N = X.shape[0]\n",
    "    Y = np.zeros((N,d))\n",
    "    for i in range(10):\n",
    "        for j in range(i):\n",
    "            prob_matrix = compute_probability_matrix(params[i][j], X)\n",
    "            k = [k for k in range(N) if prob_matrix[k][0] > 0.8]\n",
    "            l = [l for l in range(N) if prob_matrix[l][1] > 0.8]\n",
    "            Y[k,i] += 1\n",
    "            Y[l,j] += 1\n",
    "            \n",
    "    return np.argmax(Y, axis=1)\n",
    "\n",
    "Y_testp = predict_pairwise(params, X_test)\n",
    "Y_trainp = predict_pairwise(params, X_train)\n",
    "\n",
    "test_accuracy = np.mean(Y_testp==Y_test)*100\n",
    "train_accuracy = np.mean(Y_trainp==Y_train)*100\n",
    "print(\"Train Accuracy %f, \\tTest Accuracy %f\"%(train_accuracy,test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_Kaggle=pd.read_csv('test.csv', sep=',').as_matrix()\n",
    "X_Kaggle = np.column_stack((X_Kaggle, np.ones((28000,1))))\n",
    "Y_Kaggle = predict_pairwise(params, X_Kaggle)\n",
    "dataframe = pd.DataFrame({\"ImageId\":np.arange(28000)+1,\"Label\":Y_Kaggle})\n",
    "dataframe.to_csv('submit_pairwise.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
