{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data from train file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def load_mnist_data():\n",
    "    \n",
    "    data=pd.read_csv('train.csv', sep=',').as_matrix()\n",
    "    data.shape\n",
    "    N = data.shape[0]\n",
    "    n_train = int((N*70)/100)\n",
    "    X_train = data[:n_train,1:]\n",
    "    Y_train = data[:n_train,0]\n",
    "\n",
    "    X_test = data[n_train:,1:]\n",
    "    Y_test = data[n_train:,0]\n",
    "\n",
    "    C = 10\n",
    "    d = X_train.shape[1]\n",
    "    return data, N, n_train, X_train, Y_train, X_test, Y_test, C, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, N, n_train, X_train, Y_train, X_test, Y_test, C, d = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-10\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        \n",
    "        x[ix] = x[ix]+h\n",
    "        \n",
    "        random.setstate(rndstate)\n",
    "        fx1,grad1 = f(x)\n",
    "        numgrad = (fx1-fx)/h\n",
    "        \n",
    "        x[ix] = x[ix]-h\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print( \"Gradient check failed.\")\n",
    "            print( \"First gradient error found at index %s\" % str(ix))\n",
    "            print( \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "        \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print( \"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def network_forward_backward_pass(X, y, W1, b1, W2, b2, reg = 1e-3):\n",
    "    N = X.shape[0]\n",
    "    H = np.dot(X, W1) + b1\n",
    "    H_relu = np.maximum(H, 0)\n",
    "    scores = np.dot(H_relu, W2) + b2\n",
    "    \n",
    "    if y is None:\n",
    "        return scores\n",
    "    \n",
    "    weights = scores - np.max(scores, axis=1, keepdims=True)\n",
    "    exp = np.exp(weights)\n",
    "    prob_matrix = exp/np.sum(exp, axis=1, keepdims=True)\n",
    "    loss = -np.sum(np.log(prob_matrix[np.arange(N), y.reshape(1,-1)]))\n",
    "    loss /= N\n",
    "    loss += 0.5 * reg * (np.sum(W1*W1) + np.sum(W2*W2))\n",
    "    \n",
    "    #print(prob_matrix[np.arange(N), y.reshape(1,-1)].shape)\n",
    "    prob_matrix[np.arange(N), y.reshape(1,-1)] -= 1\n",
    "    dW2 = H_relu.T.dot(prob_matrix)\n",
    "    db2 = np.sum(prob_matrix, axis=0)\n",
    "    dX = prob_matrix.dot(W2.T)\n",
    "    dH = dX*(H>0)\n",
    "    dW1 = X.T.dot(dH)\n",
    "    db1 = np.sum(dH, axis=0)\n",
    "    \n",
    "    dW2 /= N\n",
    "    db2 /= N\n",
    "    dW1 /= N\n",
    "    db1 /= N\n",
    "    \n",
    "    dW2 += reg * W2\n",
    "    dW1 += reg * W1\n",
    "    \n",
    "    return loss, dW1, db1, dW2, db2\n",
    "\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    return np.argmax(network_forward_backward_pass(X, None, W1, b1, W2, b2), axis=1)\n",
    "\n",
    "predict(X[1:5], W1, b1, W2, b2)\n",
    "#network_forward_backward_pass(X[1:5], y[1:5], W1, b1, W2, b2)\n",
    "#gradcheck_naive(lambda t : network_forward_backward_pass(X[1:5], y[1:5], W1, b1, W2, t), b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Cost: 2.315556\t Val cost 2.315919\n",
      "Train Cost: 0.488772\t Val cost 0.487256\n",
      "Train Cost: 0.390508\t Val cost 0.385654\n",
      "Train Cost: 0.353073\t Val cost 0.344732\n",
      "Train Cost: 0.320935\t Val cost 0.311123\n",
      "Train Cost: 0.307649\t Val cost 0.297533\n",
      "Train Cost: 0.312502\t Val cost 0.299256\n",
      "Train Cost: 0.291057\t Val cost 0.273614\n",
      "Train Cost: 0.272406\t Val cost 0.253935\n",
      "Train Cost: 0.272910\t Val cost 0.249243\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHI9JREFUeJzt3WtsXPl93vHvb+68DSleREokdV1Jq8tKu1pKXtdN4sQN\n4nWLuEX7Ii6atEELw67dOkWA1s2LFghQFAWKIE0TJFg7TprWdQo4RuvaTpy42cY26r1Iq71YS2ml\n1a54v0i8Du8z8+uLGUqUlhJJacgzc+b5AAcz58yfM88OVs+Zc/5zMXdHRETCJRJ0ABERKT2Vu4hI\nCKncRURCSOUuIhJCKncRkRBSuYuIhJDKXUQkhFTuIiIhpHIXEQmhWFAP3Nra6gcOHAjq4UVEKtLF\nixdvuXvbRuMCK/cDBw5w4cKFoB5eRKQimdnNzYzTaRkRkRBSuYuIhJDKXUQkhFTuIiIhpHIXEQkh\nlbuISAip3EVEQqjiyr2v91Ve/dLnmc9MBR1FRKRsVVy53x68zrnB/0rf268GHUVEpGxVXLnvefJD\nAMzcULmLiDxIxZV7+94DTJDGRt4KOoqISNmquHK3SISB1FFaZnuDjiIiUrYqrtwB5nadoDvbx/Li\nQtBRRETKUkWWe7z7GeKWo++KvlVSRGQ9FVnu7UfPAzD5rspdRGQ9FVnunQePM+s1+PAbQUcRESlL\nFVnukWiU/sQTNE5pUlVEZD0VWe4AM7tOsG/lBrlsNugoIiJlp2LLPbL3DDW2zMB1nZoREblfxZZ7\n69FzANy69krASUREyk/Flnv3kadZ9DgrA3rlLiJyv4ot93g8QV/8IA2Tl4OOIiJSdiq23AEm08fp\nXr6O5/NBRxERKSsVXe7sOUOaeYZvXg06iYhIWanoct91uAeA0asvB5xERKS8VHS57zveQ9YjLPVf\nCjqKiEhZqehyT9XU0R/dR+1tTaqKiKxV0eUOMN7wJHsXrwUdQ0SkrFR8uefbn6KVKW4N3Qw6iohI\n2aj4ck8ffBaAwSuaVBURWVXx5d59ovCD2Qs3LwacRESkfFR8uTc0NtNve0nc0qSqiMiqii93gLG6\no3TM64NMIiKrQlHuy7ufYq+PMT0xHnQUEZGyEIpyr99/FoD+t38UcBIRkfKwYbmbWbeZvWhmvWZ2\n2cy+sM4YM7PfMrPrZvammZ3dnrjr6zxemFTNvP/aTj6siEjZim1iTBb4VXd/zcwagItm9hfu/vaa\nMc8DR4rLh4DfLV7uiObdnYzSQmzsrZ16SBGRsrbhK3d3H3b314rXZ4FeoPO+YZ8E/sgLXgKazGxP\nydM+xFDtMdoymlQVEYEtnnM3swPAM8D9nxjqBPrXrA/wwR0AZvZpM7tgZhfGx0s7+bnYcpLu3ADz\nmemS3q+ISCXadLmbWT3wJ8CvuPvM/Tev8yf+gQ3uL7h7j7v3tLW1bS3pBmr2nSViTl/vqyW9XxGR\nSrSpcjezOIVi/6q7f2OdIQNA95r1LmDo8eNtXseT5wGYvnFhJx9WRKQsbebdMgb8PtDr7r/xgGHf\nBH6p+K6Z54Bpdx8uYc4NtXceYpI0NvLmTj6siEhZ2sy7ZT4C/CLwlpm9Xtz2a8A+AHf/PeA7wCeA\n68A88Mulj/pwFokwkHyC5pkrO/3QIiJlZ8Nyd/cfsv459bVjHPhcqUI9qkzzKY4NfZXlpUUSyVTQ\ncUREAhOKT6iuSnSdIWE5+q7oGyJFpLqFqtx3Hyt8bmriXU2qikh1C1W5dx48QcZr8KHXNx4sIhJi\noSr3SDRKX+IwjdO9QUcREQlUqModYHbXcfYt3yCXzQYdRUQkMKEr98jep6m1JQbe1ZeIiUj1Cl25\ntx45B8D4O68EnEREJDihK/euI0+z5HFWBjWpKiLVK3TlHk8k6YsfoGHy7Y0Hi4iEVOjKHWAifZzu\npWt4Ph90FBGRQISy3G3PaRqZY7jvWtBRREQCEcpybzpcmFQduXr/b4qIiFSHUJb7vid7yHqEpb5L\nQUcREQlEKMs9VVtPf7SbmonLQUcREQlEKMsd4FbDMToX3gk6hohIIEJb7rn207Qxya3hvqCjiIjs\nuNCWe/rAswAM9mpSVUSqT2jLvftE4bvd5/teCziJiMjOC225NzS1MGgdJMd/HHQUEZEdF9pyBxit\nO0b7/NWgY4iI7LhQl/ty2yk6fZTpiVtBRxER2VGhLve6/YVJ1f7elwJOIiKys0Jd7p3HC5Oqmfcu\nBpxERGRnhbrcm9u7GKOZ6Jh+lUlEqkuoyx1guOYobZkrQccQEdlRoS/3hdZTdOcGmJ+bCTqKiMiO\nCX25p/Y9Q9ScvrdfDTqKiMiOCX257zlWmFSduqFJVRGpHqEv991dh5minsjIG0FHERHZMaEvd4tE\nGEgeYdeMJlVFpHqEvtwBMs0n2Z99n+WlpaCjiIjsiKoo93jX0yQsS99VfUOkiFSHqij39qPnAZi4\nrnfMiEh1qIpy33voFHOeIj+sSVURqQ5VUe6RaJT+xCEap94OOoqIyI6oinIHmG46wb7ld8nlckFH\nERHZdlVT7pG9T1NnSwxc15eIiUj4VU25tx45B8D4NU2qikj4VU25dx19hmWPsTJwKegoIiLbbsNy\nN7OvmNmYma37S9Nm9lEzmzaz14vLvyl9zMcXTyTpix+gflKTqiISfpt55f6HwMc3GPMDd3+6uPz6\n48faHhPp43QvXcPz+aCjiIhsqw3L3d2/D0zsQJbt13GaJjIM918POomIyLYq1Tn3D5vZG2b2p2Z2\n8kGDzOzTZnbBzC6Mj4+X6KE3b9fhHgBGrry8448tIrKTSlHurwH73f0M8J+B//mgge7+grv3uHtP\nW1tbCR56a7qPnyPnxlL/6zv+2CIiO+mxy93dZ9w9U7z+HSBuZq2PnWwbpGobGIh2UXN73blhEZHQ\neOxyN7MOM7Pi9fPF+7z9uPe7XcYbnmTPwjtBxxAR2VaxjQaY2deAjwKtZjYA/FsgDuDuvwf8PeCz\nZpYFFoBfcHfftsSPKbf7NO3Tf8H4SD9tHd1BxxER2RYblru7f2qD238b+O2SJdpmDQefhWsw1PuK\nyl1EQqtqPqG6qvtE4Qez52/qB7NFJLyqrtwbmloZtHYS45pUFZHwqrpyBxirO0b7/NWgY4iIbJuq\nLPeltqfo8hGmJ8v2TT0iIo+lKsu9bv9ZAPp7Xwo4iYjI9qjKcu88XphUnX3vtYCTiIhsj6os9+b2\nbsbZRWz0zaCjiIhsi6osd4ChmqO0ZjSpKiLhVLXlvtByku5cP/Nzs0FHEREpuaot99S+s8Qsz83e\nC0FHEREpuaot9z3HzgMwfUM/mC0i4VO15b67+wjT1GPDbwUdRUSk5Kq23C0SYSD5BLtmeoOOIiJS\nclVb7gCZ5pPsz77P8tJS0FFEREqqqss93vk0SVvh5tVLQUcRESmpqi733cVJ1Yl39Y4ZEQmXqi73\nvQdPMe9J8kP6wWwRCZeqLvdILEZf4jCNU5pUFZFwqepyB5hpOs7+5evkcrmgo4iIlEzVl3tk79PU\n2SID7+qXmUQkPKq+3FuO9AAw9o4+qSoi4VH15d519CzLHmVlQJOqIhIeVV/u8USK/tgB6icvBx1F\nRKRkqr7cASbSx+lauobn80FHEREpCZU7wJ7TNDPLcP+7QScRESkJlTvQdKgwqTp89ZWAk4iIlIbK\nHeg+fo68G0t9+sFsEQkHlTuQqkvTH+2i5rYmVUUkHFTuRbfqj7F34Z2gY4iIlITKvSjXfpp2bjM+\nMhB0FBGRx6ZyL2o4cBaAwd6XA04iIvL4VO5FXSefA2Bek6oiEgIq96KGpjaGbTeJMX2BmIhUPpX7\nGiN1x2ifvxp0DBGRx6ZyX2O59RTdPsz05ETQUUREHovKfY3a/c8C0KdJVRGpcCr3NTqPfwiA2fcu\nBpxEROTxqNzXaO7Yxy2aiI2+GXQUEZHHsmG5m9lXzGzMzNZ9G4kV/JaZXTezN83sbOlj7pzhmqO0\nZq4EHUNE5LFs5pX7HwIff8jtzwNHisungd99/FjBmW85xb5cP/PzmaCjiIg8sg3L3d2/Dzzs7SOf\nBP7IC14CmsxsT6kC7rTUvmeIWZ6bb18IOoqIyCMrxTn3TqB/zfpAcVtF6jh2HoCpGyp3EalcpSh3\nW2ebrzvQ7NNmdsHMLoyPj5fgoUtvd/dRZqjDhjWpKiKVqxTlPgB0r1nvAobWG+juL7h7j7v3tLW1\nleChS88iEQaST7BrpjfoKCIij6wU5f5N4JeK75p5Dph29+ES3G9gMrtOsj/7HsvLy0FHERF5JJt5\nK+TXgB8Bx8xswMz+sZl9xsw+UxzyHeAGcB34EvBPty3tDol1PU3KVrh59fWgo4iIPJLYRgPc/VMb\n3O7A50qWqAzsPnoeLsDE9VfgqfNBxxER2TJ9QnUdew89xYInyA29EXQUEZFHonJfRyQWoz9xiMap\nt4OOIiLySFTuDzDddIJ9y++Sy+WCjiIismUq9wewPWdosAUGblwOOoqIyJap3B+g5cg5AMbeeTXg\nJCIiW6dyf4Cuo2dZ8SjL/ZeCjiIismUq9weIJ2voj+2nflKTqiJSeVTuDzGRfpLupWt4Ph90FBGR\nLVG5P4TvOUMzMwwPvBd0FBGRLVG5P0TTocIPZg9feSngJCIiW6Nyf4ju4+fJu7HYr++YEZHKonJ/\niFRdI4PRvdTcWvfnY0VEypbKfQPj9U+yZ+GdoGOIiGyJyn0D2d1PsYdbjI+u+/sjIiJlSeW+gYaD\nhUnVwd6XA04iIrJ5KvcNdJ14DoC5mxcDTiIisnkq9w007NrNiLWRGNekqohUDpX7JozUHqN97mrQ\nMURENk3lvglLbafoyg8zPTkRdBQRkU1RuW9C3f6zRMzpu/JK0FFERDZF5b4Je48XJlVnb2hSVUQq\ng8p9E5rb9zFBI9GxN4OOIiKyKSr3zTBjsOYoLbOaVBWRyqBy36SFlpPsz/UxPz8XdBQRkQ2p3Dcp\n2X2WuOW42avz7iJS/lTum9Rx7DwAU+/qB7NFpPyp3Ddp975jzFILI5pUFZHyp3LfJItEGEg+wa7p\n3qCjiIhsSOW+BbO7TrI/+x7Ly8tBRxEReSiV+xbEO89QY8vcvKqf3ROR8qZy34K2ox8C4Pb1CwEn\nERF5OJX7Fuw9/BSLHic/pFfuIlLeVO5bEInF6UscJj2lSVURKW8q9y2aaTzOvuXr5HL5oKOIiDyQ\nyn2LbO8Z0jZP/w29eheR8qVy36KWJ3oAGHtHP5gtIuVL5b5FnceeZcWjrAxoUlVEypfKfYviyVoG\nYvuom7gcdBQRkQdSuT+C2+njdC9dw/OaVBWR8rSpcjezj5vZVTO7bmZfXOf2f2Rm42b2enH5J6WP\nWkY6nqKFaYYG3g86iYjIujYsdzOLAr8DPA+cAD5lZifWGfo/3P3p4vLlEucsK02HzgEwclWTqiJS\nnjbzyv08cN3db7j7MvDHwCe3N1Z56zp+jrwbi32Xgo4iIrKuzZR7J9C/Zn2guO1+f9fM3jSzr5tZ\nd0nSlalUfRND0b2kbv046CgiIuvaTLnbOtv8vvX/DRxw99PA94D/su4dmX3azC6Y2YXx8fGtJS0z\n4/XH2L/wY777zf/OjZHbuN//lIiIBCe2iTEDwNpX4l3A0NoB7n57zeqXgP+w3h25+wvACwA9PT0V\n3Ya1Z/42DT/4AT/32meZu5jkh9HTTOz5KZrOfIKzp0/TkIoHHVFEqpht9IrTzGLAO8DHgEHgVeDv\nu/vlNWP2uPtw8frfAf6Vuz/3sPvt6enxCxcq/Ktzl+cYe/N7TL7xbZqH/4q27AgA1/Kd9NY/R+6J\nn+WJZz/Gye42IpH1DoBERLbGzC66e8+G4zZzOsHMPgH8JhAFvuLu/87Mfh244O7fNLN/D/w8kAUm\ngM+6+5WH3Wcoyn0td5ZHrzL06v+C69+jc/o14mTJeIpX7DRjHT9J+tTz9Jw5xe6GVNBpRaRClbTc\nt0Poyv1+Sxmm3/4/TLzxLRoH/orm7CgAvflu3qr5EMuHPsbhZ36as4d2k4xFAw4rIpVC5V5O3MmP\n9jJ66Vvkrn6XjqlLxMgx4zX8iNMMtf516k5+nHOnT3KgpRYzncIRkfWp3MvZ4gwL7/wlty99i4aB\n/0vjSuGdQ5fz+7mY6GFx/89w4OmP8uEj7ZqYFZF7qNwrhTuMXmbijW+zfOW7tE1eIkqeaa/lh/nT\nvN/8EWpO/BznTj7Jyb1pTcyKVDmVe6VamGLl+otMvv4tavtepH6l8C7TN/MHeTn6LLP7fpr9T/0E\nP3GsXROzIlVI5R4G+TyMvkXm8p+y+Paf0TzxBhHyTHg938+f5lrDh0k8+bMcP3yQjsYUHY0pWuuS\nenUvEmIq9zCanyD/7otMv/ltku+/SO3KBHk3bvgeBr2VIW9hhFYyqQ5W6vdAuptEcxetuxrpSKdo\nT6fYU9wJpOJ6h45IJVK5h10+D8Ovs3Tlz1nofx2bHiAxN0TN8u0PDB33NEPeyrC3MOQtDHoL0/F2\nluv24k1d1DZ10N5YS3tjio50ofw70ima6xJ6545ImdlsuW/m6wekHEUi0HmWZOdZkmu3Z5dgZhCm\nB2C6cNk42UftRB9PTA+SyFwmnpsvjJ0rLCuDMYZ9F0PFV/+93sKwtzBmbSzW7cHSXTTsaikUfzpF\ne2PxCCCdYnc6qffpi5QhlXvYxJLQfKiwFCWKC1B4d87idKH8ZwZhup/49CBdU/20T/bzzPT7xOde\nIuLZwvglYBwy47WFV/35Zoa8lavFo4Ahb2W+pgPSe2lO19Nan6StIUlrfaJ4mbxz2VQT13yAyA5R\nuVcbM6hpKiwdp+5sjsDdI4B8DjKjxVf+/TAzSP30AEem+zk0NQBTrxNbXHP6Jw/5KWNmupFxGhnJ\nNTLqTQx5I294E2PexLg3MWFNZGt3U9vQRGtDitb6JK0NCdrW7AC0IxApDZW7fFAkCum9haX73J3N\nxpr/YVYWYGaoUP7Tg0Sm+2maHaEpM8YTmRF89gaWGcPyy/fedxaWppJMThdKfzjXyGi+kWvexP+j\niXFvZMybmLRd5GtbaU7XFXYC6+wIVncG2hGIfJDKXR5NvAZaDheW+1hxwR0WJiEzVjgSKC7JzCgd\ns6N0ZEZ5KjOGz14jsjj5gfvJZ43MVCMT002M5hsZzKUZyzfxZnEHMF7cGdyyZpK1jYWjgYYkDckY\nyXiEmniUmniUVDxKTSJKMhahJrFmWzx6d1wiSipWvIxHScUjJKIRTShLxVK5y/Yxg9rmwrL7yfWH\nFBeySzA3DrN3dwKRzCjpzCjp2VEOZEbxzPuQGcVyyx+4n5VcgqnZZm7PNjHrKeY9ybzHmcvHyeQS\nLJBgxpOMkmCxuCx4kgUSLJBk0YvbSLLghcslSxCJp6iJx+4U/tqdQ+rOTiJyz7bVHUneIZfPk8vf\nd+lONu/k8/ddupPNOTl3cvnCsrotX/yb3P2Lr7OtuD2bc8ygPhkjXRMnnYqRTsVpSBXWG+6sx0nX\nxAqXqdiddU2UVzaVu5SHWBIauwrLA9w5GlicKhwNzI7cOSqIZ0Zoy4zRlhmF5TlYmYOVeVhZwFcW\nYGUByy1tOZZjrHiS5ZUkS9kkS4upws7BkywQZ8ETzHmCuXyCTD7BXL6wbbI4hW04Ebx4T4X1mDlJ\ng4hB1CBqTsQgYk7UCvMf0YgTgbvbWR1TvN2cSPHvbHW9OMYihXWLO06EKa/ndqaOW9N1jGZr6F+u\nZXg5xe18A9PUsXR3uv0eiViE9Grhr7dzWN1p1MRoSMbv7jCKl/WJmE6XBUjlLpXFDGp2FZa2Y5v7\nk9Ur+VxhrmBloVD82cU7O4C7lwv3jLGVBRIr8yRWFqhfd/w8rNy+c90faSdSPH5xAyKF/8a8FS69\n2NZ3rtu911lzO6u3R+7ens8WTo3df7Szps/zsRpWkrtYjqdZjDUyH00zG0kzY/VMej2T+TrGc3WM\nzdYycruG68u1DC4lyKw8/Fc67xw1FHcI9ckYdckY9cXir08V1hvWbk9GqU/GqUtGaShe1qd0FPEo\nVO5SPSJRSNYXlm1iUNiJZBeLGyLcKd11i3gHXtm6F3ZCC5MwP1G4XJi4sx5ZmCS5MElyfoKGhUlY\nuAmZycLt+ez69xkFr20gn9xFNtnEcqLxzo4hE0kzYw1MUc9Evo5buTrGszXczia5NZegfyLK3HKO\nzGKWueXcpv4T4lG7u3NYXe7fOWxiezxaeL7XfnTzns9x+tqrvu6Ye//WP7DdH3Afa6/WJKLb/o2v\nKneRUotEIVEXdIq7zAp5EnUPPe31Ae6wNHvPjqCwYyhct4VJogsTRFd3DLNDxbFT3FuB94nEINkA\nzQ14soFcvIFsvJ6VWB1L0XoWI3XMWy1zVsuc1zBDDdP5GqZySaayKcazSW5nk0xkcvRNzBd2Ekub\n31GUg8/81GG++Pz681ClonIXkfWZQSpdWHYd2Pzf5fOFeZE1OwIWp2BpprCzWF0WZ7ClWWJLM8QW\nx0kt3aBh9bbVI5+HicQgmYa6BmhO48l6cvEGVuL1LEfrWIzUshCpZ95qyFBLxlMsWRK3CG4x8kTu\nXPdIFLfVJVK4JFrcHllzW3E9EruzfvdobM0pwDVHZGuPzVY3n9iT3vzz+YhU7iJSWpHI3XdJPars\ncnEnMHPf5WzhE9ZrdxLF22xpltjcCLGlWWqWZmlcmoVHmER/JBYp7GwsWriMRNZcL15a5O71s/8Q\n9n1+WyOp3EWk/MQSEGuBupbHu5/s0t0dwOJMYd1zhXmRfHbN9Vzxeva+9bXj8muuF8d5fs3frDdu\ndfuacZ6D+t2leZ4eQuUuIuEVSxaWutagk+y4h7+XSUREKpLKXUQkhFTuIiIhpHIXEQkhlbuISAip\n3EVEQkjlLiISQip3EZEQMveHfMHPdj6w2Thw8xH/vBW4VcI4lU7Px730fNyl5+JeYXg+9rt720aD\nAiv3x2FmF9y9J+gc5ULPx730fNyl5+Je1fR86LSMiEgIqdxFREKoUsv9haADlBk9H/fS83GXnot7\nVc3zUZHn3EVE5OEq9ZW7iIg8RMWVu5l93Myumtl1M/ti0HmCZGbdZvaimfWa2WUz+0LQmYJmZlEz\nu2Rm3wo6S9DMrMnMvm5mV4r/j3w46ExBMbN/Ufw38mMz+5qZpYLOtN0qqtzNLAr8DvA8cAL4lJmd\nCDZVoLLAr7r7ceA54HNV/nwAfAHoDTpEmfhPwJ+5+5PAGar0eTGzTuCfAz3ufgqIAr8QbKrtV1Hl\nDpwHrrv7DXdfBv4Y+GTAmQLj7sPu/lrx+iyFf7ydwaYKjpl1AX8T+HLQWYJmZmngJ4HfB3D3ZXef\nCjZVoGJAjZnFgFpgKOA8267Syr0T6F+zPkAVl9laZnYAeAZ4OdgkgfpN4F8C+aCDlIFDwDjwB8XT\nVF82s7qgQwXB3QeB/wj0AcPAtLv/ebCptl+llbuts63q3+5jZvXAnwC/4u4zQecJgpn9LWDM3S8G\nnaVMxICzwO+6+zPAHFCVc1RmtovCEf5BYC9QZ2b/INhU26/Syn0A6F6z3kUVHF49jJnFKRT7V939\nG0HnCdBHgJ83s/cpnK77GTP7b8FGCtQAMODuq0dyX6dQ9tXobwDvufu4u68A3wD+WsCZtl2llfur\nwBEzO2hmCQqTIt8MOFNgzMwonFPtdfffCDpPkNz9X7t7l7sfoPD/xV+6e+hfnT2Iu48A/WZ2rLjp\nY8DbAUYKUh/wnJnVFv/NfIwqmFyOBR1gK9w9a2afB75LYcb7K+5+OeBYQfoI8IvAW2b2enHbr7n7\ndwLMJOXjnwFfLb4QugH8csB5AuHuL5vZ14HXKLzD7BJV8ElVfUJVRCSEKu20jIiIbILKXUQkhFTu\nIiIhpHIXEQkhlbuISAip3EVEQkjlLiISQip3EZEQ+v9PuZfxAizwwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d239e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.934116\t Testing Accuracy 0.926270\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "y = Y_train\n",
    "X_val = X_test\n",
    "y_val = Y_test\n",
    "\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "H = 15\n",
    "C = 10\n",
    "std = 1e-2\n",
    "W1 = std*np.random.randn(D, H)\n",
    "b1 = np.zeros(H)\n",
    "\n",
    "W2 = std*np.random.randn(H, C)\n",
    "b2 = np.zeros(C)\n",
    "\n",
    "def gradient_descent_nn(X, y, X_val, y_val, W1, b1, W2, b2, batch_size = 200, \n",
    "                       lr = 1e-3, reg = 0.5e-2, num_iters = 100, print_iters = 10):\n",
    "    it = 0\n",
    "    N = X.shape[0]\n",
    "    stats_train = []\n",
    "    stats_val = []\n",
    "    best_val, best_params = -1, None\n",
    "    while it < num_iters:\n",
    "        indices = np.random.choice(N, batch_size)\n",
    "        X_batch = X[indices]\n",
    "        y_batch = y[indices]\n",
    "        loss, dW1, db1, dW2, db2 = network_forward_backward_pass(X_batch, y_batch, W1, b1, W2, b2, reg)\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "        \n",
    "        if it%print_iters == 0:\n",
    "            #train_acc = np.mean(y == predict(X, W1, b1, W2, b2))\n",
    "            #val_acc = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "            loss_val = network_forward_backward_pass(X, y, W1, b1, W2, b2, reg)[0]\n",
    "            loss_train = network_forward_backward_pass(X_val, y_val, W1, b1, W2, b2, reg)[0]\n",
    "            \n",
    "            test_accuracy = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "            if test_accuracy > best_val:\n",
    "                best_val = test_accuracy\n",
    "                best_params = (W1, b1, W2, b2)\n",
    "            \n",
    "            stats_train.append(loss_train)\n",
    "            stats_val.append(loss_val)\n",
    "            print(\"Train Cost: %f\\t Val cost %f\"%(loss_train, loss_val))\n",
    "            \n",
    "        it += 1\n",
    "    \n",
    "    return W1, b1, W2, b2, stats_train, stats_val, best_val, best_params\n",
    "\n",
    "W1, b1, W2, b2, training_error, testing_error, best_val, best_params = gradient_descent_nn(X, y, X_val, y_val, \n",
    "                                                                    W1, b1, W2, b2, \n",
    "                                                                    num_iters = 1000, print_iters = 100)\n",
    "\n",
    "x = range(len(training_error))\n",
    "plt.plot(x, training_error, x, testing_error)\n",
    "plt.show()\n",
    "\n",
    "test_accuracy = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "train_accuracy = np.mean(y == predict(X, W1, b1, W2, b2))\n",
    "print(\"Training Accuracy %f\\t Testing Accuracy %f\"%(train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for hidden size : 800, reg : 0.000010, lr : 0.001000, batch_size : 200\n",
      "Train Cost: 2.311556\t Val cost 2.312876\n",
      "Train Cost: 0.444912\t Val cost 0.009691\n",
      "Train Cost: 0.612937\t Val cost 0.001856\n",
      "Train Cost: 0.692384\t Val cost 0.000972\n",
      "Train Cost: 0.743995\t Val cost 0.000652\n",
      "Train Cost: 0.783001\t Val cost 0.000493\n",
      "Train Cost: 0.811884\t Val cost 0.000397\n",
      "Train Cost: 0.837040\t Val cost 0.000335\n",
      "Train Cost: 0.859262\t Val cost 0.000293\n",
      "Train Cost: 0.877138\t Val cost 0.000262\n",
      "Train Cost: 0.893554\t Val cost 0.000238\n",
      "Train Cost: 0.907907\t Val cost 0.000220\n",
      "Train Cost: 0.920791\t Val cost 0.000205\n",
      "Train Cost: 0.932948\t Val cost 0.000193\n",
      "Train Cost: 0.943899\t Val cost 0.000183\n"
     ]
    }
   ],
   "source": [
    "def train_with_hyperparameters(H, reg, lr, batch_size):\n",
    "    \n",
    "    print(\"Testing for hidden size : %d, reg : %f, lr : %f, batch_size : %d\"%(H, reg, lr, batch_size))\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    H = 15\n",
    "    C = 10\n",
    "    std = 1e-2\n",
    "    W1 = std*np.random.randn(D, H)\n",
    "    b1 = np.zeros(H)\n",
    "\n",
    "    W2 = std*np.random.randn(H, C)\n",
    "    b2 = np.zeros(C)\n",
    "\n",
    "    W1, b1, W2, b2, train_err, testing_err = gradient_descent_nn(X, y, X_val, y_val, W1, b1, W2, b2, \n",
    "                                                                 batch_size = batch_size, lr = lr, reg = reg, \n",
    "                                                                 num_iters = 1000000, print_iters = 50000)\n",
    "    \n",
    "    x = range(len(training_error))\n",
    "    plt.plot(x, training_error, x, testing_error)\n",
    "    plt.show()\n",
    "\n",
    "    test_accuracy = np.mean(y_val == predict(X_val, W1, b1, W2, b2))\n",
    "    train_accuracy = np.mean(y == predict(X, W1, b1, W2, b2))\n",
    "    print(\"Training Accuracy %f\\t Testing Accuracy %f\"%(train_accuracy, test_accuracy))\n",
    "    \n",
    "    params = [W1, b1, W2, b2, H, reg, lr, batch_size]\n",
    "    return params, train_accuracy, test_accuracy\n",
    "\n",
    "Hs = {800} #, 10, 15, 20, 25}\n",
    "regs = {1e-5, 1e-4} #, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1}\n",
    "lrs = {1e-3}\n",
    "batches = {200}\n",
    "\n",
    "best_val = -1\n",
    "best_params = None\n",
    "\n",
    "for H in Hs:\n",
    "    for reg in regs:\n",
    "        for lr in lrs:\n",
    "            for batch_size in batches:\n",
    "                params, train_accuracy, test_accuracy = train_with_hyperparameters(H, reg, lr, batch_size)\n",
    "                if test_accuracy > best_val:\n",
    "                    best_val = test_accuracy\n",
    "                    best_params = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_Kaggle=pd.read_csv('test.csv', sep=',').as_matrix()\n",
    "Y_Kaggle = predict(X_Kaggle, W1, b1, W2, b2)\n",
    "dataframe = pd.DataFrame({\"ImageId\":np.arange(28000)+1,\"Label\":Y_Kaggle})\n",
    "dataframe.to_csv('submit_nn.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92277777777777781"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
